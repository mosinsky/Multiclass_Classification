{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "# import en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# stopwords = nlp.Defaults.stop_words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn import preprocessing\n",
    "from nltk import ngrams\n",
    "from nltk import FreqDist\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# nltk.download('all')\n",
    "from nltk import pos_tag\n",
    "import joblib\n",
    "from textaugment import Wordnet\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#options\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
    "    text = re.sub('\\[.*\\]','', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge cleaning functions to one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "    tokens_without_stopwords = remove_stopwords(tokens)\n",
    "    preprocessed_text = ' '.join(tokens_without_stopwords)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Restore basic forms of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data \n",
    "path = \"data/complaints.json\"\n",
    "open_path = open(path) \n",
    "read_data = json.load(open_path)\n",
    "df=pd.json_normalize(read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "### dataset - https://www.kaggle.com/datasets/abhishek14398/automatic-ticket-classification-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/complaints.json\"\n",
    "open_path = open(path) \n",
    "read_data = json.load(open_path)\n",
    "df=pd.json_normalize(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick only columns which are needed\n",
    "df = df[['_source.complaint_what_happened', '_source.issue', '_source.product', '_source.sub_product']]\n",
    "\n",
    "#rename for normal names\n",
    "df = df.rename(columns={'_source.complaint_what_happened': 'complaint',  '_source.issue' : 'issue', '_source.product': 'product','_source.sub_product': 'sub_product'})\n",
    "\n",
    "# drop columns with blank description\n",
    "df[df['complaint']==''] = np.nan\n",
    "df = df[~df['complaint'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['issue'] + ' / ' + df['product'] + ' / ' + df['sub_product']\n",
    "df.drop(['issue', 'product', 'sub_product'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = df # replace to have possibility to load back original data\n",
    "data['text_clean'] = data['complaint'].apply(lambda x: preprocess_text(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: lemmatize(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: remove_pos_tags(x))\n",
    "\n",
    "data['text_clean'] = data['text_clean'].str.lower()\n",
    "data['text_clean'] = data['text_clean'].str.replace('xxxx','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "### in this dataset I have too much categories, so my plan is to change quantity of categories to 5 using NFM method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')\n",
    "\n",
    "dtm = tfidf.fit_transform(data['text_clean']) # document term metrix\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')\n",
    "dtm = tfidf.fit_transform(data['text_clean']) # document term metrix\n",
    "feature_names = np.array(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "num_topics = 5  # You can adjust the number of topics as needed\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "# nmf_model = NMF(n_components=num_topics)\n",
    "nmf_matrix = nmf_model.fit_transform(dtm)\n",
    "\n",
    "# Normalize the NMF matrix\n",
    "nmf_matrix_normalized = normalize(nmf_matrix, axis=1)\n",
    "\n",
    "# Assign topics to documents\n",
    "data['topic'] = nmf_matrix_normalized.argmax(axis=1)\n",
    "\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    topic_words_idx = nmf_model.components_[topic].argsort()[-15:][::-1]\n",
    "    topic_words = [feature_names[i] for i in topic_words_idx]\n",
    "    \n",
    "    print(f\"Top 15 words for Topic {topic}:\\n\")\n",
    "    print(topic_words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPPING AND SAVING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic_mapping = {\n",
    "    0: 'Banking and Account activities',\n",
    "    1: 'Credit/debits Cards',\n",
    "    2: 'Other',\n",
    "    3: 'Reporting/information',\n",
    "    4: 'Loans/Mortgages'\n",
    "}\n",
    "\n",
    "#Replace Topics with Topic Names\n",
    "data['topic'] = data['topic'].map(topic_mapping)\n",
    "data.to_csv('data/data.csv', index=False) # saving dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic_counts = data['topic'].value_counts() # get the topic for each category\n",
    "max_count = topic_counts.max() # max items for main topic\n",
    "\n",
    "augmented_data = []\n",
    "wordnet_aug = Wordnet(v=True) # use Wordnet for synonims\n",
    "\n",
    "# loop for creating additional data\n",
    "for topic, count in topic_counts.items():\n",
    "    if count < max_count:\n",
    "        topic_data = data[data['topic'] == topic]\n",
    "        samples_needed = max_count - count # find how many text data we need to add for each group\n",
    "        \n",
    "        # perform augmenation\n",
    "        augmented_sentences = []\n",
    "        while len(augmented_sentences) < samples_needed:\n",
    "            augmented_sentence = wordnet_aug.augment(topic_data['text_clean'].sample().iloc[0])\n",
    "            augmented_sentences.append(augmented_sentence)\n",
    "        \n",
    "        # create new dataframe with merged newly created samples and topics\n",
    "        augmented_df = pd.DataFrame({'topic': [topic] * len(augmented_sentences), 'text_clean': augmented_sentences})\n",
    "        \n",
    "        # add samples from list to dataframe\n",
    "        augmented_data.append(augmented_df)\n",
    "\n",
    "# merge newly created dataframe with samples and oryginal data\n",
    "augmented_data = pd.concat([data] + augmented_data, ignore_index=True)\n",
    "\n",
    "# print the results\n",
    "print(\"Orginal data counts :\")\n",
    "print(data.topic.value_counts())\n",
    "print(\"\\nAugmented data counts :\")\n",
    "print(augmented_data.topic.value_counts())\n",
    "\n",
    "augmented_data.to_csv('data/augmented_data.csv', index=False) # saving dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "df_scores = df = pd.DataFrame(columns=['Model', 'Training Score', 'Test Score']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_classification_results(model, X_train, y_train, X_test, y_test):\n",
    "    global df_scores\n",
    "    \n",
    "    try:\n",
    "        # Training data predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        \n",
    "        # Test data predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Accuracy Scores\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        # Classification reports\n",
    "        train_classification_report = classification_report(y_train, y_train_pred)\n",
    "        test_classification_report = classification_report(y_test, y_test_pred)\n",
    "    \n",
    "    #inputs for some methods needs to be as array instead of string\n",
    "    except:\n",
    "        y_train_pred = model.predict(X_train.toarray())\n",
    "        \n",
    "        # Test data predictions\n",
    "        y_test_pred = model.predict(X_test.toarray())\n",
    "\n",
    "         # Accuracy Scores\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Classification reports\n",
    "        train_classification_report = classification_report(y_train, y_train_pred)\n",
    "        test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "    #print scores\n",
    "    print(\"\\nTraining Accuracy:\", train_accuracy)\n",
    "    print(\"\\nTraining Classification Report:\")\n",
    "    print(train_classification_report)\n",
    "    \n",
    "    print(\"Testing Accuracy:\", test_accuracy)\n",
    "    print(\"\\nTesting Classification Report:\")\n",
    "    print(test_classification_report)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    unique_classes = np.unique(np.concatenate([y_train, y_test]))\n",
    "    cm = confusion_matrix(y_test, y_test_pred, normalize='true')  # Normalize confusion matrix\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Change color map to Greens\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Greens\", xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "    \n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "data = pd.read_csv('data/augmented_data.csv')\n",
    "data['text_clean'] = data['text_clean'].fillna('')\n",
    "data[data['text_clean']==''] = np.nan\n",
    "data = data[~data['text_clean'].isnull()]\n",
    "\n",
    "training_data = data[['text_clean','topic']]\n",
    "X = training_data['text_clean']\n",
    "y = training_data['topic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "cv = CountVectorizer()\n",
    "X_vec = cv.fit_transform(X)\n",
    "tfidf_t = TfidfTransformer()\n",
    "X_tfidf = tfidf_t.fit_transform(X_vec)\n",
    "joblib.dump(cv, 'pre-trained_models/cv.joblib')\n",
    "joblib.dump(tfidf_t, 'pre-trained_models/tfidf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=40, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=40,solver='liblinear')\n",
    "log_reg.fit(X_train,y_train)\n",
    "joblib.dump(log_reg, 'pre-trained_models/logistic_regression.joblib')\n",
    "display_classification_results(log_reg, X_train, y_train, X_test, y_test,)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
