{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
    "    text = re.sub('\\[.*\\]','', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge cleaning functions to one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "    tokens_without_stopwords = remove_stopwords(tokens)\n",
    "    preprocessed_text = ' '.join(tokens_without_stopwords)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore basic forms of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    sent = [token.lemma_ for token in doc if token.text not in STOP_WORDS]\n",
    "\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    sent = [token.text for token in doc if token.tag_ == 'NN']\n",
    "\n",
    "    return ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augemntation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get synonyms with WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return list(synonyms)\n",
    "\n",
    "# perform aAugmentation\n",
    "def augment_text(sentence):\n",
    "    words = sentence.split()\n",
    "    augmented_words = []\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            augmented_words.append(random.choice(synonyms))\n",
    "        else:\n",
    "            augmented_words.append(word)\n",
    "    return ' '.join(augmented_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset - https://www.kaggle.com/datasets/abhishek14398/automatic-ticket-classification-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"data/complaints.json\"\n",
    "open_path = open(path) \n",
    "read_data = json.load(open_path)\n",
    "df=pd.json_normalize(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick only columns which are needed\n",
    "df = df[['_source.complaint_what_happened', '_source.issue', '_source.product', '_source.sub_product']]\n",
    "\n",
    "#rename for normal names\n",
    "df = df.rename(columns={'_source.complaint_what_happened': 'complaint',  '_source.issue' : 'issue', '_source.product': 'product','_source.sub_product': 'sub_product'})\n",
    "\n",
    "# drop columns with blank description\n",
    "df[df['complaint']==''] = np.nan\n",
    "df = df[~df['complaint'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new column with merged 3 columns for category and drop previous columns\n",
    "df['category'] = df['issue'] + ' / ' + df['product'] + ' / ' + df['sub_product']\n",
    "df.drop(['issue', 'product', 'sub_product'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df\n",
    "data['text_clean'] = data['complaint'].apply(lambda x: preprocess_text(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: lemmatize(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: remove_pos_tags(x))\n",
    "\n",
    "data['text_clean'] = data['text_clean'].str.lower()\n",
    "data['text_clean'] = data['text_clean'].str.replace('xxxx','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')\n",
    "dtm = tfidf.fit_transform(data['text_clean']) # document term metrix\n",
    "feature_names = np.array(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words for Topic 0:\n",
      "\n",
      "['account', 'check', 'money', 'bank', 'deposit', 'fund', 'day', 'branch', 'transfer', 'number', 'business', 'transaction', 'chase', 'customer', 'claim']\n",
      "\n",
      "\n",
      "Top 15 words for Topic 1:\n",
      "\n",
      "['credit', 'card', 'report', 'inquiry', 'account', 'score', 'company', 'information', 'chase', 'limit', 'application', 'debt', 'letter', 'year', 'balance']\n",
      "\n",
      "\n",
      "Top 15 words for Topic 2:\n",
      "\n",
      "['payment', 'balance', 'month', 'pay', 'statement', 'fee', 'time', 'day', 'mortgage', 'date', 'credit', 'year', 'auto', 'account', 'error']\n",
      "\n",
      "\n",
      "Top 15 words for Topic 3:\n",
      "\n",
      "['charge', 'card', 'dispute', 'fee', 'transaction', 'purchase', 'merchant', 'claim', 'service', 'refund', 'fraud', 'time', 'email', 'statement', 'balance']\n",
      "\n",
      "\n",
      "Top 15 words for Topic 4:\n",
      "\n",
      "['loan', 'mortgage', 'modification', 'home', 'property', 'year', 'letter', 'document', 'rate', 'request', 'time', 'foreclosure', 'refinance', 'information', 'sale']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "num_topics = 5  # You can adjust the number of topics as needed\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "# nmf_model = NMF(n_components=num_topics)\n",
    "nmf_matrix = nmf_model.fit_transform(dtm)\n",
    "\n",
    "# Normalize the NMF matrix\n",
    "nmf_matrix_normalized = normalize(nmf_matrix, axis=1)\n",
    "\n",
    "# Assign topics to documents\n",
    "data['topic'] = nmf_matrix_normalized.argmax(axis=1)\n",
    "\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    topic_words_idx = nmf_model.components_[topic].argsort()[-15:][::-1]\n",
    "    topic_words = [feature_names[i] for i in topic_words_idx]\n",
    "    \n",
    "    print(f\"Top 15 words for Topic {topic}:\\n\")\n",
    "    print(topic_words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "0    5139\n",
       "3    4924\n",
       "1    4817\n",
       "4    3818\n",
       "2    2374\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPPING AND SAVING DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = {\n",
    "    0: 'Banking and Account activities',\n",
    "    1: 'Credit/debits Cards',\n",
    "    2: 'Other',\n",
    "    3: 'Reporting/information',\n",
    "    4: 'Loans/Mortgages'\n",
    "}\n",
    "\n",
    "#Replace Topics with Topic Names\n",
    "data['topic'] = data['topic'].map(topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/data.csv', index=False) # saving dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_counts = data['topic'].value_counts()\n",
    "max_count = topic_counts.max() # max items for main topic\n",
    "\n",
    "\n",
    "# Loop for augmenting data\n",
    "augmented_data = []\n",
    "for topic, count in topic_counts.items():\n",
    "    if count < max_count:\n",
    "        topic_data = data[data['topic'] == topic]\n",
    "        samples_needed = max_count - count\n",
    "        \n",
    "        # perform augmenation\n",
    "        augmented_sentences = []\n",
    "        while len(augmented_sentences) < samples_needed:\n",
    "            augmented_sentence = augment_text(topic_data['text_clean'].sample().iloc[0])\n",
    "            augmented_sentences.append(augmented_sentence)\n",
    "        \n",
    "        # create new dataframe with merged newly created samples and topics\n",
    "        augmented_df = pd.DataFrame({'topic': [topic] * len(augmented_sentences), 'text_clean': augmented_sentences})\n",
    "\n",
    "        # add samples from list to dataframe\n",
    "        augmented_data.append(augmented_df)\n",
    "\n",
    "# Merge augmented data with original data\n",
    "augmented_data = pd.concat([data] + augmented_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data.to_csv('data/augmented_data.csv', index=False) # saving dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = augmented_data[['text_clean','topic']]\n",
    "X = training_data['text_clean']\n",
    "y = training_data['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_vec = cv.fit_transform(X)\n",
    "\n",
    "tfidf_t = TfidfTransformer()\n",
    "X_tfidf = tfidf_t.fit_transform(X_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre-trained_models/tfidf.joblib']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv, 'pre-trained_models/cv.joblib')\n",
    "joblib.dump(tfidf_t, 'pre-trained_models/tfidf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=40, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre-trained_models/logistic_regression.joblib']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=40,solver='liblinear')\n",
    "log_reg.fit(X_train,y_train)\n",
    "joblib.dump(log_reg, 'pre-trained_models/logistic_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "        \n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(train_accuracy)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
